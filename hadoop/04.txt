hadoop
---------------------
    1.API
        Configuration    // 配置类,fs.defaultFS(file://)
    2.核心配置文件
        core-site.xml    // 文件系统 + 本地临时目录 hadoop.tmp.dir
        hdfs-site.xml    // relication = 3
        mapred-site.xml    // yarn 
        yarn-site.xml    // rm
    3.blocksize
        128m
        寻址时间    ~= 10ms
        磁盘的io速率    = 100M/s
        让寻址事件是读取时间的1%
    4.centos
    5.hadoop
        hadoop/share/hadoop/common|hdfs|yarn|mapred|.../lib/jar

yum
---------------------
    管理依赖

maven
----------------------
    apache maven
    1.下载maven软件
        linux    // apache-maven-3.3.9-bin.tar.gz
        windows    // 
    2.安装
        解压即可
    3.配置环境变量
        M2_HOME=安装目录
        PATH=%M2_HOME%/bin;...

修改软件仓库
-----------------------
    1.conf/settings.xml
    2.修改本地库路径
        [默认位置]
        ~/.m2/repository    // windows C:\Users\Administrator
                            // linux :/home/centos/m2
        [conf.settings.xml]
            ...
            <localRepository>/path/to/local/repo</localRepository>
            ...
        

体验maven
----------------------
    0.查看maven的帮助(mvn help:system)
        cmd>cd bin
        cmd>mvn
    1.创建项目
        编写pom.xml(project object model)文件
        <project>
            <modelVersion>4.0.0.</modelVersion>
            <!-- groupid映射到文件夹结构,.解析成路径 -->
            <groupId>org.appache.hadoop</groupId>
            
            <!-- 工件id,对应一个文件夹 -->
            <aritfactId>org.apache.hadoop</artifacrId>
            
            <!-- 工件的版本,对应一个文件夹 -->
            <version>2.7.3</version>
        </project>

        <project>
            <modelVersion>4.0.0.</modelVersion>
            <groupId>liuqi15</groupId>
            <version>1.0.0</version>
        </project>
    2.创建文件夹
        src/main/java    // maven默认的源代码目录
    3.创建java类
        liuqi15/helloworld/HelloWorld.java
        package liuqi15.helloworld;
        public class HelloWorld {
            public static void main(String[] args) {
                System.out.println("hello world");
            }
        }
    4.编译java源码
        cmd
        tree /F
        mvn clean compile
    5.打包
        mvn clean package
        java -cp helloworld-1.0.0.jar liuqi15.helloworld.HelloWorld


搭建maven私服
-----------------------------
    1.下载maven web程序(war)
        nexus.war
    2.部署war文件
        复制nexus.war到${tomcat_home}/webapps/即可
    3.配置私服的仓库地址
        ${tomcat_home}\webapps\nexus\WEB-INF\classes\nexus.properties
        // nexus-work=${user.home}/sonatype-work/nexus
        nexus-work=h:/maven-repo/nexus
        runtime=${bundleBasedir}
        nexus-app=${runtime}
    4.启动tomcat
        cmd
        Tomcat 9.0\bin\startup.bat
    5.通过浏览器访问nexus
        http://localhost:8080/nexus/
    6.复制文件到相应目录下

配置maven的settings文件,指定私服的地址
--------------------------------------
    1.编辑settins.xml文件
        [${m2_home/conf/settings/xml}]
        <localRepository>xxx</localRepository>
        ~/.m2/settings.xml指定用户

maven常用命令
------------------
    cmd>mvn clean // 清除targe目录
    cmd>mvn clean compile // 清除并编译
    cmd>mvn clean package // 清除并打包

使用archetype生成项目骨架
--------------------------
    cmd>mvn archetype:generate
    cmd>mvn archetype:generate -DarchetypeCatalog=internal

eclipse中配置maven
--------------------------
    1.eclipse -> 首选项 -> Maven -> User Settings
        Global Settings : C:\myprograms\apache-maven-3.6.0\conf\setttings.xml
        User Settings : C:\myprograms\apache-maven-3.6.0\conf\settings.xml 
        自动解析本地仓库目录 : C:\Users\liuqi15\.m2\repository
    2.OK

缺少winutils.exe文件以及xxx.dll文件
--------------------------
    1.解压bin.rar文件,将bin/*所有文件附睾到${hadoop_hom)/bin下
    2.设置windows的环境变量
      PATH=${hadoop_home}/bin;${hadoop_hom}/sbin;...
    3.将bin.rar,bin/*.dll + winutils.ext文件放到系统目录下
      系统目录:
              C:\Windows\System32
              C:\Windows\SysWOW64

windows上配置hadoop_home环境变量
-------------------------------
    hadoop_home=d:\downloads\hadoop-2.7.3
    path=%hadoop_home%\bin;%hadoop_home%\sbin;...

使用eclipse创建maven项目
--------------------------
    1.eclipse -> new project -> maven project -> 指定location -> 选择工件类型 archetype 中使用 archetype-quickstart

设置文件块
--------------------------
    1.只有文件才有副本和块的概念.
      文件blocksize默认是128，
,最好块大小是1m.blocksize >= 最小值.
    2.block默认是是128m
    [hdfs-site.xml]
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>2</value>
        </property>
        <property>
            <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
            <value>false</value>
        </property>
        <!-- -->
        <property>
            <name>dfs.namenode.fs-limits.min-block-size</name>
            <value>1024</value>
        </property>
        <property>
            <name>dfs.blocksize</name>
            <value>2k</value>
        </property>
    </configuration>
    3.分发配置文件
        $>xsync.sh hdfs-site.xml
    4.通过hdfs命令查询key
        $>hdfs getconf -confKey dfs.blocksize
        $>hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size
    5.重启hdfs
　　　　$>stop-dfs.sh
        $>start-dfs.sh
    6.put文件到hdfs
        $>hdfs dfs -put 1.txt hadoop
    7.使用API创建文件制定副本和块大小设置
        private void printlnPath(FileSystem fs, Path p) {
            try {
                // 输出路径
                System.out.println(p.toUri().toString());
                if (fs.isDirectory(p)) {
                    FileStatus[] files = fs.listStatus(p);
                    if (files != null && files.length > 0) {
                        for(FileStatus f : files) {
                            Path p0 = f.getPath();
                            printlnPath(fs, p0);
                        }
                    }
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
       }

        


文件最少的副本数
---------------------
    dfs.namenode.replication.min=1    // dfs.replication.min(过时了)
    dfs.replication.max=512 


failover    // 容灾
fault tolerance // 容错

512进行一次校验,Crc32实现校验算法,通过本地实现,(hadoop/bin/hadoop.dll)

chunk    // 512byte
checksum    // 36byte = 4(校验和类型) +32(检验和值) 
pack    // 每个packet包含多个chunk

createPackage(packSize, chunkPerPacket, bytesCyrBlock, currentSeqno++, false)

剖析文件写入过程
----------------------
    1.DistributedFiledSystem dfs = FileSystem.get(conf);
      //RPC:remote procedure call,远程过程调用 


