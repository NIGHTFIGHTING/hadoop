大数据
---------------
    0.分布式
        由分布在**不同主机**上的进程**协同**在一起构成整个应用
        block.
    1.存储
        分布式存储
    2.计算
        分布式计算


管理hadoop
----------------
    1.配额
        空间配额
        目录配额
    2.快照
        snapshot
    3.回收站
        trash    //
    4.上下线
         dfs.hosts    // hdfs
         dfs.hosts.exclude    // hdfs

GFS
----------------
    google file system         

Mysql:OLTP

MR
----------------
    MapReduce
    映射和化简
    编程模型

 
eclipse显示Override/Implement Methods
alt>s>v
ctrl+3
右键->source->Override/Implement Methods

使用mr计算年度的最高气温
-----------------------------
    1.1901.gz + 1902.gz
    2.编写mapper
        [MaxTempMapper.java]
        package com.it18zhang.mapreduce;
        
        import java.io.IOException;
        
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.LongWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Mapper;
        /**
         * MR:Map
         **/
        public class MaxTempMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        
            // 缺失常量
            public static final int MISSING = 9999;
            @Override
            protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
                    throws IOException, InterruptedException {
                // 取得一整行文本
                String line = value.toString();
                // 提取年分值
                String year = line.substring(15, 19);
                // 定义气温变量
                int airTemperature;
                // 取出气温
                if (line.charAt(87) == '+') {
                    airTemperature = Integer.parseInt(line.substring(88, 92));
                } else {
                    airTemperature = Integer.parseInt(line.substring(87, 92)); 
                }
                // 提取质量
                String quality = line.substring(92, 93);
                
                if (airTemperature != MISSING && quality.matches("[01459]")) {
                    context.write(new Text(year), new IntWritable(airTemperature));
                }
            }
        
        }
        
    3.编写reducer
        [MaxTempReducer.java]
        package com.it18zhang.mapreduce;

        import org.apache.hadoop.mapreduce.Reducer;
        import java.io.IOException;
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.Text;
        
        /**
         * MR:Reduce
         **/
        
        public class MaxTempReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        
            @Override
            protected void reduce(Text key, Iterable<IntWritable> values,
                    Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
                // 最大值
                int maxValue = Integer.MIN_VALUE;
                // 提取年份的最大值
                for (IntWritable value : values) {
                    maxValue = Math.max(maxValue, value.get());
                }
                // output key maxValue
                context.write(key, new IntWritable(maxValue));
            }
        
        }
    4.编写
        [App.java]
        public class App {
  
         public static void main(String[] args) {
             if (args.length != 2) {
                 System.err.println("Usage: MaxTemperature <input path> <output path>");
             }
             try {
                 // 创建配置对象
                 Configuration conf = new Configuration();
                 FileSystem fs = FileSystem.get(conf);
                 if (fs.exits(new Path(args[1])) {
                     fs.delete(new Path(args[1]), true);
                 }
                 // 创建job对象
                 Job job = Job.getInstance(conf);
                 
                 // 设置jar搜索类
                 job.setJarByClass(App.class);
                 // 设置作业名称
                 
                 // 添加输入路径
                 FileInputFormat.addInputPath(job, new Path(args[0]));
                 
                 // 设置输出路径
                 FileOutputFormat.setOutputPath(job, new Path(args[1]));
                 //设置Mapper类型
                 job.setMapperClass(MaxTempMapper.class);
                 //设置Reducer类型
                 job.setReducerClass(MaxTempReducer.class);
                 //设置输出key类型
                 job.setOutputKeyClass(Text.class);
                 //设置输出Value类型
                 job.setOutputValueClass(IntWritable.class);
                 Iterator<Entry<String, String>> it = conf.iterator();
                 while (it.hasNext()) {
                     Entry<String, String> e = it.next();
                     System.out.println(e.getKey() + "=" + e.getValue());
                 }
                 System.exit(job.waitForCompletion(true) ? 0 : 1);
             } catch (Exception e) {
                 
             }
         }
     
     }
     5.导出jar包
         mapreduce-0.0.1-SNAPSHOT.jar
     6.put天气文件到hdfs
     7.复制mapreduce-0.0.1-SNAPSHOT.jar到共享目录
     8.启动yarn进程
         $>start-yarn.sh
     9.验证
         $>xcall.sh jps
     10.执行hadoop jar进行mr作业
         $>hadoop jar mapreduce-0.0.1-SNAPSHOT.jar com.it18zhang.mapreduce.App /user/centos/hadoop/data /user/centos/hadoop/out

$>yarn-daemon.sh stop nodemanager


word count
----------------------------
hello world1 hello world2 hello world1 hello world3
hello world1 hello world2 hello world1 hello world3
hello world1 hello world2 hello world1 hello world3
hello world1 hello world2 hello world1 hello world3

map(keyin, valuein, keyout, valueout) {
    map() {
        ...
        String line = valuein.toString();
        String[] arr = line.split(" ");
        
        for (String w : arr) {
            context.write(new Text(w), new IntWritable(1));
        }
    }
}


reduce
------------------------
    public void reduce(Text key, Iterable<IntWritable>, Context ctx) {
        for() {
            ++;
            ctx.write();
        }
    }
hadoop jar wc-0.0.1-SNAPSHOT.jar com.it18zhang.wc.App /user/centos/hadoop/data2 /user/centos/hadoop/out2


调整集群
--------------------------
    1.配置206是2nn,注释掉dfs.hosts属性和dfs.hosts.exclude属性
        [hdfs-site.xml]
        <configuration>
            <property>
                <name>dfs.replication</name>
                <value>2</value>
            </property>
        
            <property>
               <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
               <value>false</value>
            </property>
            <property>
               <name>dfs.blocksize</name>
               <value>128m</value>
            </property>
            <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:///home/centos/hadoop/dfs/name1,file:///home/centos/hadoop/dfs/name2</value>
            </property> 
            <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:///home/centos/hadoop/dfs/data1,file:///home/centos/hadoop/dfs/data2</value>
            </property> 
            <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>s206:50090</value>
            </property> 
            <!---
            <property>
                <name>dfs.hosts</name>
                <value>/soft/hadoop/etc/hadoop/datanodes.host</value>
            </property> 
            <property>
                <name>dfs.hosts.exclude</name>
                <value>/soft/hadoop/etc/hadoop/datanodes.exclude.host</value>
            </property> 
            <property>
               <name>dfs.namenode.fs-limits.min-block-size</name>
               <value>1024</value>
            </property>
            -->
        </configuration>
    2.yarn-site.xml文件注释掉inpath等属性
        <configuration>
            <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>s201</value>
            </property>
            <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
            </property>
            <!--
            <property>
                <name>yarn.resourcemanager.bodes.include-path</name>
                <value>/soft/hadoop/etc/hadoop/nms.host</value>
            </property>
            <property>
                <name>yarn.resourcemanager.bodes.exclude-path</name>
                <value>/soft/hadoop/etc/hadoop/nms.exclude.host</value>
            </property>
            -->
        </configuration>
    3.slaves配置文件如下
        [slaves]
        s202
        s203
    4.注释掉回收站
        <configuration>
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://s201/</value>
            </property>
            <property>
                <name>hadootmp.dir</name>
                <value>/home/centos/hadoop</value>
            </property>
            <!--
            <property>
                <name>fs.trash.interval</name>
                <value>1</value>
            </property>
            <property>
                <name>fs.trash.checkpoint.interval</name>
                <value>1</value>
            </property>
            <property>
                <name>net.topology.node.switch.mapping.impl</name>
                <value>com.it18zhang.myhadoop_maven.MyDNSToSwitchMapping</value>
            </property>
            -->
        </configuration>


windows配置ip地址到主机名的映射
-------------------
C:\Windows\System32\drivers\etc\hosts


combiner
--------------------------
    map侧的reduce过程,但不是所有,r都适用
    combiner就是reducer
    job.setCombinerClass(WCReducer.class);
